import base64
import sys
import vertexai
from vertexai.generative_models import GenerativeModel, ChatSession, Part, FinishReason
import vertexai.preview.generative_models as generative_models
from google.cloud import bigquery


def generate(model_api: str = None, expert_model_api: str = None, system_instruction: str = None, user_input: str = None, temperature: float = None, top_p: float = None, top_k: float = None, candidate_count: int = 1, max_output_tokens: int = 2048, stop_sequences: str = None, table_id: str = None):
  
  vertexai.init(project="andrewcooley-test-project", location="us-central1")
  client = bigquery.Client(project="andrewcooley-test-project", location="US")

  generation_config = {
    "max_output_tokens": max_output_tokens,
    "temperature": temperature,
    "top_p": top_p,
    "top_k": top_k,
    "candidate_count": candidate_count,
    "stop_sequences": stop_sequences
  }

  safety_settings = {
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
  }

  model = GenerativeModel(model_api, system_instruction=[system_instruction])
  responses = model.generate_content(
      [user_input],
      generation_config=generation_config,
      safety_settings=safety_settings,
      stream=True,
  )

  output =""

  print("Output: ")

  for response in responses:
    print(response.text, end="")
    output += response.text

  vertexai.init(project="cloud-llm-preview1", location="us-central1")

  expert = GenerativeModel(expert_model_api)

  expert_rating = int(expert.generate_content([f"""You are an expert at explaining machine learning concepts to non-experts. You will be given a response generated by another system that has attempted to do what you are an expert at. You will grade it on a scale from 1 to 5.
                              
                              You are primarily grading the response on how well the concept is explained to a non-expert. More about the grading scale: 1 = poor, 2 = needs improvement, 3 = satisfactory, 4 = good, 5 = excellent.

                              Below is the response generated by another system.

                              Reponse: {output}

                              Return only the numeric grade (1-5) in your response, no explanation.
                              
                              """]).text)

  rows_to_insert = [{"model_api": f"{model_api}", "expert_model_api": f"{expert_model_api}", "system_instruction": f"{system_instruction}", "user_input": f"{user_input}", "temperature": generation_config["temperature"], "top_p": generation_config["top_p"], "top_k": generation_config["top_k"], "candidate_count": generation_config["candidate_count"], "max_output_tokens": generation_config["max_output_tokens"], "output": f"{output}", "expert_rating": expert_rating}]

  errors = client.insert_rows_json(table_id, rows_to_insert)

  print(f"\n\nExpert rating: {expert_rating}")
  
  if errors == []:
    print("\n\nPrompt has been logged into BQ.")
  else:
    print("Encountered errors while inserting rows: {}".format(errors))  

if __name__ == "__main__":
  # Get the function parameters from the command line
  model_api = sys.argv[1]
  system_instruction = sys.argv[2]
  user_input = sys.argv[3]
  prompt = sys.argv[4]
  table_id = sys.argv[5]

  # Call the generate function with the function parameters
  generate(model_api=model_api, expert_model_api=expert_model_api, system_instruction=system_instruction, user_input=user_input, temperature=temperature, table_id=table_id)
