{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"andrewcooley-test-project\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from uuid import uuid4\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "import warnings\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.evaluation import (\n",
    "    EvalTask,\n",
    "    PromptTemplate,\n",
    "    CustomMetric,\n",
    "    make_metric,\n",
    ")\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuid(length: int = 8) -> str:\n",
    "    \"\"\"Generate a uuid of a specifed length (default=8).\"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "def print_doc(function):\n",
    "    print(f\"{function.__name__}:\\n{inspect.getdoc(function)}\\n\")\n",
    "\n",
    "\n",
    "def display_eval_report(eval_result, metrics=None):\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "\n",
    "    title, summary_metrics, report_df = eval_result\n",
    "    metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
    "    if metrics:\n",
    "        metrics_df = metrics_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in metrics_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "        report_df = report_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in report_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Display the title with Markdown for emphasis\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "\n",
    "    # Display the metrics DataFrame\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(metrics_df)\n",
    "\n",
    "    # Display the detailed report DataFrame\n",
    "    display(Markdown(f\"### Report Metrics\"))\n",
    "    display(report_df)\n",
    "\n",
    "\n",
    "def display_explanations(df, metrics=None, n=1):\n",
    "    style = \"white-space: pre-wrap; width: 800px; overflow-x: auto;\"\n",
    "    df = df.sample(n=n)\n",
    "    if metrics:\n",
    "        df = df.filter(\n",
    "            [\"instruction\", \"context\", \"reference\", \"completed_prompt\", \"response\"]\n",
    "            + [\n",
    "                metric\n",
    "                for metric in df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for col in df.columns:\n",
    "            display(HTML(f\"{col}: {row[col]}\"))\n",
    "        display(HTML(\"\"))\n",
    "\n",
    "\n",
    "def plot_radar_plot(eval_results, metrics=None):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for eval_result in eval_results:\n",
    "        title, summary_metrics, report_df = eval_result\n",
    "\n",
    "        if metrics:\n",
    "            summary_metrics = {\n",
    "                k: summary_metrics[k]\n",
    "                for k, v in summary_metrics.items()\n",
    "                if any(selected_metric in k for selected_metric in metrics)\n",
    "            }\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatterpolar(\n",
    "                r=list(summary_metrics.values()),\n",
    "                theta=list(summary_metrics.keys()),\n",
    "                fill=\"toself\",\n",
    "                name=title,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 5])), showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_bar_plot(eval_results, metrics=None):\n",
    "    fig = go.Figure()\n",
    "    data = []\n",
    "\n",
    "    for eval_result in eval_results:\n",
    "        title, summary_metrics, _ = eval_result\n",
    "        if metrics:\n",
    "            summary_metrics = {\n",
    "                k: summary_metrics[k]\n",
    "                for k, v in summary_metrics.items()\n",
    "                if any(selected_metric in k for selected_metric in metrics)\n",
    "            }\n",
    "\n",
    "        data.append(\n",
    "            go.Bar(\n",
    "                x=list(summary_metrics.keys()),\n",
    "                y=list(summary_metrics.values()),\n",
    "                name=title,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig = go.Figure(data=data)\n",
    "\n",
    "    # Change the bar mode\n",
    "    fig.update_layout(barmode=\"group\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def print_aggregated_metrics(job):\n",
    "    \"\"\"Print AutoMetrics\"\"\"\n",
    "\n",
    "    rougeLSum = round(job.rougeLSum, 3) * 100\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"The {rougeLSum}% of the reference summary is represented by LLM when considering the longest common subsequence (LCS) of words.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def print_autosxs_judgments(df, n=3):\n",
    "    \"\"\"Print AutoSxS judgments in the notebook\"\"\"\n",
    "\n",
    "    style = \"white-space: pre-wrap; width: 800px; overflow-x: auto;\"\n",
    "    df = df.sample(n=n)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"confidence\"] >= 0.5:\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"Document: {row['id_columns']['document']}\"\n",
    "                )\n",
    "            )\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"Response A: {row['response_a']}\"\n",
    "                )\n",
    "            )\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"Response B: {row['response_b']}\"\n",
    "                )\n",
    "            )\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"Explanation: {row['explanation']}\"\n",
    "                )\n",
    "            )\n",
    "            display(\n",
    "                HTML(\n",
    "                    f\"Confidence score: {row['confidence']}\"\n",
    "                )\n",
    "            )\n",
    "            display(HTML(\"\"))\n",
    "\n",
    "\n",
    "def print_autosxs_win_metrics(scores):\n",
    "    \"\"\"Print AutoSxS aggregated metrics\"\"\"\n",
    "\n",
    "    score_b = round(scores[\"autosxs_model_b_win_rate\"] * 100)\n",
    "    display(\n",
    "        HTML(\n",
    "            f\"AutoSxS Autorater prefers {score_b}% of time Model B over Model A \"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_doc(PromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    \"{system_instruction} Answer this question:{question}, and follow the requirements: {requirements}.\"\n",
    ")\n",
    "\n",
    "\n",
    "compiled_prompt = prompt_template.assemble(\n",
    "    system_instruction=\"You are a poetic assistant, skilled in explaining complex concepts with creative flair.\",\n",
    "    question=\"How does LLM work?\",\n",
    "    requirements=\"Explain concepts in great depth using simple terms, and give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\",\n",
    ")\n",
    "\n",
    "model_response = (\n",
    "    GenerativeModel(\"gemini-pro\")\n",
    "    .generate_content(str(compiled_prompt))\n",
    "    .candidates[0]\n",
    "    .content.parts[0]\n",
    "    .text\n",
    ")\n",
    "\n",
    "\n",
    "display(HTML(f\"Compiled Prompt:{compiled_prompt}\"))\n",
    "display(HTML(f\"Model Response: \"))\n",
    "Markdown(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"English\"\n",
    "\n",
    "context = [\n",
    "    \"Can someone make a Powerpoint of what we've discussed today?\",\n",
    "    \"Same here. Rome was great. I want to go back with you.\",\n",
    "    \"I got a phone call from the airplane that our flight has been canceled due to heavy rain.\",\n",
    "    \"Yeah, I've heard that. I can't believe this. I'm glad to see you overseas.\",\n",
    "    \"Let me know where we will have dinner, and I will go there with my team members.\"\n",
    "]\n",
    "\n",
    "reference = [\n",
    "    \"Following the discussion today, I would appreciate it if someone could create a PowerPoint presentation summarizing the key points covered.\",\n",
    "    \"I concur. Rome was a truly remarkable experience. I would be delighted to accompany you on a return visit.\",\n",
    "    \"As informed by the airline via phone call, our flight has been canceled due to inclement weather conditions.\",\n",
    "    \"I see. That is indeed surprising news. It's a pleasure to connect with you overseas nonetheless.\",\n",
    "    \"I'd be delighted to assist you in determining the dinner arrangements for you and your team. Kindly inform me of the designated dining location, and I will ensure our presence at the appointed time.\"\n",
    "]\n",
    "\n",
    "\n",
    "eval_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"context\": context,\n",
    "        \"instruction\": [instruction] * len(context),\n",
    "        \"reference\": reference,\n",
    "    }\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [\n",
    "\"\"\"- Safety First: Your top priority is to protect users. Do not generate responses that could be harmful, offensive, discriminatory, or expose sensitive information.\n",
    " - Identify Unsafe Inputs: Examine the input text carefully. If it contains any of the following the output \"[Error:1000]\" with the number and explanation of the reason you identify unsafe the input.\n",
    "   1. Instructions or plans related to illegal activities (theft, hacking, violence, etc.)\n",
    "   2. Insults, hateful language, or threats directed at individuals or groups\n",
    "   3. Statements that express prejudice or discrimination based on gender, race, sexual orientation, etc.\n",
    "   4. Statements that demean or express prejudice against people based on their religion, race, ethnicity, sexual orientation, etc.\n",
    "   5. Statements expressing hatred or the desire to harm others based on religion, race, ethnicity, etc.\n",
    "   6. Sexually explicit content\n",
    "   7. Exposure of private information (names, addresses, etc.)\n",
    " - Do not answer for the input text and just rewrite the input to the professional tone used when doing business in the company as an expert while trying to keep original meaning and length in {instruction}.\n",
    " - Output only the rephrased text - do not include any additional labels, metadata, tags (text, output, rephrased text).\n",
    " \n",
    " Input:\n",
    " {context}\n",
    "\n",
    " Output:\n",
    " \"\"\",\n",
    "\"You are a tone converter.  Take this statement in {instruction} and rephrase it in a profrofessional tone.  '{context}'  If you can't do that then only respond with [Error:1000]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"coherence\",\n",
    "    \"fluency\",\n",
    "    \"fulfillment\",\n",
    "    \"safety\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "gemini_model = GenerativeModel(\n",
    "    \"gemini-1.0-pro-002\", generation_config=generation_config, safety_settings=safety_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"eval-sdk-prompt-engineering\"\n",
    "\n",
    "tone_conversion_eval_task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    experiment=experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = generate_uuid()\n",
    "eval_results = []\n",
    "\n",
    "\n",
    "for i, prompt_template in tqdm(\n",
    "    enumerate(prompt_templates), total=len(prompt_templates)\n",
    "):\n",
    "    experiment_run_name = f\"eval-prompt-engineering-{run_id}-prompt-{i}\"\n",
    "\n",
    "    eval_result = tone_conversion_eval_task.evaluate(\n",
    "        prompt_template=prompt_template,\n",
    "        experiment_run_name=experiment_run_name,\n",
    "        model=gemini_model,\n",
    "    )\n",
    "\n",
    "    eval_results.append(\n",
    "        (f\"Prompt #{i}\", eval_result.summary_metrics, eval_result.metrics_table)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_result in eval_results:\n",
    "    display_eval_report(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
